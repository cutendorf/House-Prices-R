---
title: "House Price Predictions - Report"
author: "Christine Utendorf"
date: "23 May 2019"
output: html_document
---

## 0. Sources loading
First all needed libraries are loaded. This includes data.table, DT and lubridate to work with datasets (that also include dates), ggplot2, ggthemes, GGally, grid, gridExtra, plotly, leaflet and corrplot for data exploration/visualization purposes and caret and ranger for the machine learning models.

All self-programed functions such as transforming or splitting a dataset are saved within the R notbook called functions.

Furthermore, several metrics that are not default for machine learning R packages are created as functions within the R notebook metrics. 
```{r, warning=FALSE, message=FALSE}
source('notebooks/libraries.R')
source('notebooks/functions.R')
source('notebooks/metrics.R')
```

## 1. Data sets

### 1.1 Data loading
Two data sets are provided. Both cover house sales within King County, USA and each row represents a single house sale between May 2014 and May 2015. While the train data set includes the price for each sale the test data set does not. Thus we are provided with a "blind" test data set. The train data set will be used to train and tune the machine learning model, while the test data set will be used to make price predictions based on the provided data and its features.
```{r}
raw_train_data<-fread('data/house_price_train.csv', stringsAsFactors = F)
raw_test_data<-fread('data/house_price_test.csv', stringsAsFactors = F)
```

### 1.2 Data description
Consequently the train data set includes 21 columns while the test data set has only 20 columns. While the train data has 17,277 house sales, the test data has 4,320 house sales. Both data sets actually come from a complete data set that can be found on Kaggle under following [link](https://www.kaggle.com/harlfoxem/housesalesprediction). However, this data set was probably then randomly splitted into train and test data using a 80:20 split and then for the test data set the prices were removed. Thus both data sets share the same 19 house features and each sale is identfied with a numeric id.
Most of the features are directly related to the house properties such as number of bedrooms, squarefeet, waterfront, floors etc. In addition, we have information of the building year of the house and the lastest renovation (if there was any). There are also features that try to capture the overall condition of the house such as grade and condition. However, these two variables are likely to capture the same information and thus one might be redundant. Furthermore location data is provided as well as how often the place was visited. Overall, the dataset covers the most important house features that are also usually used on house sale websites.
```{r, warning=FALSE, message=FALSE}
str(raw_train_data)
str(raw_test_data)
datatable(raw_train_data)
```

##### NA detection
Overall, we see that the data does not have any missing values, which is an imporant aspect in terms of data quality.
```{r}
sum(is.na(raw_train_data))
sum(is.na(raw_test_data))
```


### 1.3 Data preparation
By looking at the data types of the several features, we can see right away that some of them are not in the correct format. Thus with the self-programmed function **prep_data several** datatypes for both data sets are changed:

* date: chr to date
* zip code: int to factor 
 
Furthermore based on the information we have some features are created with this function. These features could already be seen as part of feature engineering/creation but are important aspects espcially for the data exploration:

* renovated: if the renovation year is different from 0, then there was a renovation (1), if it is equal to 0 there was none (0)
* basement: if the variable sqft_basement is 0, then there is n basement (0), otherwise there is one (1)
* houseage: in order to interpret the building year on a better scale, the actual age of the house if calculated by taking the difference between the current year and the building year

The function is specifically programmed for these data sets and are applied to both train and test. The transformed data sets are then saved within a new variable.


There are several features that appear to be rather factor variables than numbers or integers. They represent discrete variables and should be turned into factors. Before turning them into factors, first data visualization is performed to actually see if we find linear relationship between those features and the target variable. If this is the case we can leave them as numeric or integer variable, in any other case they should be turned into factors.

##### Train data
```{r}
prep_train_data <- prep_data(raw_train_data)
str(prep_train_data)
```
##### Test data
```{r}
prep_test_data <- prep_data(raw_test_data)
str(prep_test_data)
```


### 1.4 Data visualization
One of the most important things in machine learning is to get familiar with the data set. Data exploration and in particular data visualization is very helpful in this regard. This part will only look at the train data set since the model is trained on it and the whole purpose is to create a model that has good generalization power. Thus all the information in the test data set is not relevant at this point.

#### Distribution
In order to have a general feeling for the train dataset, a table is created that shows the general distribution of all numeric variables. Especially the price quantiles can help later in the visualization when price bins are needed.
With regard to the boolean variables we already see from their means that only 1% of the houses is on the waterfront, only 4% of the houses were renovated and 39% of the houses have a basement.
```{r}
df <- as.data.frame(prep_train_data)
df <- df[ , !(names(df) %in% c('id', 'date', 'zipcode', 'lat', 'long'))]
distribution <- as.data.frame(t(sapply(df, quantile)))
distribution$Mean <- sapply(df, mean)
distribution$SD <- sapply(df, sd)
datatable(round(distribution, 2))
```

#### 1.4.1 Price distribution
Since price is the target variable the first step is too look at the distribution of this continous variable.
```{r,warning=FALSE, message=FALSE}
price_dist <- ggplot(prep_train_data, aes(x = price)) +    
              geom_histogram(alpha = 0.8, fill='greenyellow') +
              labs(x= 'Price',y = 'Count', title = 'Price distribution') + 
              theme_bw()+  
              theme(text = element_text(face = "bold"),
                    panel.border = element_blank(), 
                    panel.grid.major = element_blank(),
                    panel.grid.minor = element_blank(),
                    axis.line = element_blank(),
                    axis.ticks = element_blank(),
                    legend.position="none")


ggplotly(price_dist)
```

Since there are some houses that sold for very high prices (>2 million) the distribution chart does not reveal much due to the high skewness to the right. Thus the distribution chart is now cut off at 2 mio in order to gain a better picture of the price distribution

```{r,warning=FALSE, message=FALSE}
price_dist_cut <- ggplot(prep_train_data, aes(x = price)) +    
              geom_histogram(alpha = 0.8, fill='greenyellow') +
              scale_x_continuous(limits=c(0,2e6)) +
              labs(x= 'Price',y = 'Count', title = 'Cutted price distribution') + 
              theme_bw()+  
              theme(text = element_text(face = "bold"),
                    panel.border = element_blank(), 
                    panel.grid.major = element_blank(),
                    panel.grid.minor = element_blank(),
                    axis.line = element_blank(),
                    axis.ticks = element_blank(),
                    legend.position="none")



ggplotly(price_dist_cut)
```

The distribution is still skewed to the right but now we see that most houses sold between 200k and 1mio. By setting the cut-off point at 2mio, 157 house sales (0.09%) were not included due to their high selling prices. Nevertheless it is important to determine why some of the houses sold for such a large amount.

#### 1.4.2 Discrete variables
Since there are several integer variables that are discrete rather than continous variables, it makes sense to look at them in a separate way than continous variables. First all the names of discrete variables are stored within a variable. Then a new dataframe is created that only stores the discrete variables. Since these variables stored discrete values they are transformed into factores and afterwards are melted.

```{r}
discVar <- c("bedrooms", "bathrooms", "floors", "waterfront", "view", "condition", "grade", "renovated", "basement")

df_disc <- prep_train_data[, ..discVar]
df_disc <- sapply(df_disc, as.factor)
df_disc <- as.data.frame(melt(df_disc))
df_disc$value <- factor(df_disc$value, levels=sort(as.numeric(levels(df_disc$value))), ordered=TRUE)
```

```{r}
options(repr.plot.width = 24, repr.plot.height = 8)

disc_dist <- ggplot(df_disc, aes(value)) +
      geom_bar(aes(fill = Var2)) + 
      scale_y_continuous(breaks = scales::pretty_breaks(n = 5)) +
      scale_x_discrete(expand = c(0,0)) +
      facet_wrap(~Var2, scales = "free", nrow = 3) +
      scale_fill_tableau() +
      ggtitle("Count of each discrete variable") +
      labs(fill = "", x = "", y = "") +
      theme_minimal() +
      theme(text = element_text(face = "bold"),
            legend.position = "none",
            axis.text.x = element_text(angle = 0),
            panel.grid.major = element_blank(),
            panel.grid.minor = element_blank(),
           plot.title = element_text(hjust = 0.5)) 

disc_dist
```

There are several features that have a highly unbalanced distribution such as views (many never viewed = 0), waterfront (most not on waterfront) and renovated (most not renovated.). Something else we see right away by looking at the histograms is that it seems that for bedroom and bathroom there are some questionable values. While there are houses without a bathrooms (= 0), another house appears to have 33 bedrooms. Thus it makes sense to further look at outliers in this case.

```{r}
options(repr.plot.width = 10, repr.plot.height = 10)

disc_box <- ggplot(df_disc, aes(Var2, as.numeric(value))) +
                geom_boxplot(aes(fill = Var2)) +
                scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
                scale_x_discrete(expand = c(0,0)) +
                facet_wrap(~Var2, scales = "free", ncol = 1) +
                scale_fill_tableau() +
                ggtitle("Distribution of each discrete variable") +
                labs(fill = "", x = "", y = "") +
                coord_flip() +
                theme_light() +
                theme(text = element_text(face = "bold"),
                      legend.position = "none",
                      axis.text.x = element_blank(),
                      panel.grid.major = element_blank(),
                      panel.grid.minor = element_blank(),
                      plot.title = element_text(hjust = 0.5),
                      strip.background = element_blank(),
                      strip.text.x = element_blank())

disc_box
```

We see again that especially bedroom and bathroom are variables with several outliers. Thus we will take a closer look at it in the data cleaning part.


#### 1.4.3 Continous variables
Similar to the discrete variables, we also have a look at the several continous variables. Six of them are all related to squarefeet measures while the seventh is the newly created houseage. In order to plot them properly a new melted dateframe is created.

```{r, warning=FALSE, message=FALSE}
contVar <- c("sqft_living", "sqft_lot", "sqft_above", "sqft_basement", "sqft_living15", "sqft_lot15", "houseage")
df_cont <- prep_train_data[,..contVar]
df_cont <- as.data.frame(melt(df_cont))
```

We see that all of these continous variables are skewed to the right. Especially the one once related to lot sizes showing that there are many with very little lot sizes and very few with large lot sizes. For sqft of the basement we see similar to the variable of having a basement or not that a large proportion of the houses have no basement and thus a sqft size of 0 for basement. Houseage is actually the variable that is more evenly distributed. We will take a closer look in the following at the year built variable.

```{r}
options(repr.plot.width = 12, repr.plot.height = 6)

cont_dist <- ggplot(df_cont, aes(value)) +
        geom_density(aes(fill = variable)) +
        facet_wrap(~variable, scales = "free") +
        labs(x = "", y = "", fill = "") +
        theme_minimal() +
        scale_fill_tableau() +
        ggtitle("Distribution of each continous variable") +
        theme(text = element_text(face = "bold"),
              legend.position = "none",
              panel.grid.major = element_blank(),
              panel.grid.minor = element_blank(),
             plot.title = element_text(hjust = 0.5))

cont_dist
```

Since we know we have highly skewed variables it make sense to take a look at their boxplots. However, using non-normalized boxplots does not reveal much due to the large number of outliers

```{r}
cont_box <- ggplot(df_cont, aes(variable, value)) +
          geom_boxplot(aes(fill = variable)) +
          coord_flip() +                                
          scale_fill_tableau() +
          labs(x = "", y = "") +
          theme_minimal() +
          theme(text = element_text(face = "bold"),
                legend.position = "none",
                panel.grid.major = element_blank(),
                panel.grid.minor = element_blank(),
               plot.title = element_text(hjust = 0.5),
               axis.text.x = element_blank())                                
                            
                      
cont_box
```

Thus the variables are normalized using the max-min normalizer. Now the boxplots reveal a bit more but still many variables have a high number of outliers.

```{r, warning=FALSE, message=FALSE}

df_cont_norm <- prep_train_data[,..contVar]
df_cont_norm <- as.data.frame(apply(df_cont_norm, 2,function(x)((x - min(x))/(max(x)-min(x)))))
df_cont_norm <- as.data.frame(melt(df_cont_norm))

cont_box_norm <- ggplot(df_cont_norm, aes(variable, value)) +
              geom_boxplot(aes(fill = variable)) +
              coord_flip() +                                
              scale_fill_tableau() +
              labs(x = "", y = "") +
              theme_minimal() +
              theme(text = element_text(face = "bold"),
                    legend.position = "none",
                    panel.grid.major = element_blank(),
                    panel.grid.minor = element_blank(),
                   plot.title = element_text(hjust = 0.5),
                   axis.text.x = element_blank())                                
                            
                      
cont_box_norm
```

#### 1.4.4 Building year
Overall, we see that the number of houses built per year seems to rise. However especially during economic downturns such as the 1930s and the years following the financial crisis in 2008. However, it is likely that this does not reflect the actual building pattern in this area since in specific years some houses from a specific time could be torn down or a particularly popular. However, there might be actual price trends that could be related to the building year, since some century building style could be more popular than others.

```{r}
options(repr.plot.width = 10, repr.plot.height = 5)

year_plot <- ggplot(prep_train_data, aes(yr_built)) +
        geom_bar(fill = "coral4") +
        scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
        scale_x_continuous(breaks = scales::pretty_breaks(n = 10), expand = c(0,0)) +
        ggtitle("Houses built per year") +
        theme_minimal() +
        theme(text = element_text(face = "bold"),
             plot.title = element_text(hjust = 0.5))

ggplotly(year_plot)
```

While a simple scatterplot does not help to acutally detect any pattern, a smoothing average line actually reveals that prices for houses built between 1940 and 1980 do not seem to sell at high prices on average. While there seem to be tweo houses built before 1940 that sold for very high prices (>7mio), most of the high priced houses were built after 1990.

```{r, warning=FALSE, message=FALSE}
options(repr.plot.width = 10, repr.plot.height = 5)

y1 <- ggplot(prep_train_data, aes(yr_built, price)) +
        geom_point(colour = "greenyellow") +
        ggtitle("Prices throughout construction years") +
        scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
        scale_y_continuous(breaks = scales::pretty_breaks(n = 10)) +
        theme_minimal() +
        theme(text = element_text(face = "bold"),
             axis.text.x = element_blank(),
             axis.title.x = element_blank())

y2 <- ggplot(prep_train_data, aes(yr_built, price)) +
        geom_smooth(se = FALSE, colour = "greenyellow") +
        ggtitle("Average prices throughout construction years") +
        scale_x_continuous(breaks = scales::pretty_breaks(n = 10)) +
        scale_y_continuous(breaks = scales::pretty_breaks(n = 8)) +
        theme_minimal() +
        theme(text = element_text(face = "bold"))

output1 <- grid.arrange(y1, y2)
```

#### 1.4.5 Location data
In order to determine if the location of a house actually effects its price all houses were assigned to five different price bins. While 300k (25% quantile), 450k (50% quantile) and 600k (75% quantile) are based on the price distribution within the data set, another cut was chosen at 2mio based on the highly skewed price histogram. While less expensive houses are grey and the most expensive houses appear in black. Thus creating a good contrast. The other the price bins become more red the more expensive they are.

We see overall especially houses close to the water are very expensive and that several neighborhood show many houses within the same price bin. Thus including the zip code and location data within the machine learning model might be beneficial. Especially when using a random forest this might work out well.

```{r}
location_data <- data.frame(prep_train_data)
location_data$PriceBin<-cut(location_data$price, c(0,300000,450000,650000,2000000,9000000))

center_lon = median(location_data$long,na.rm = TRUE)
center_lat = median(location_data$lat,na.rm = TRUE)

factpal <- colorFactor(c("#BDBDBD","#58FA82", "#F4FA58", "#FAAC58", "#FA5882", "#1C1C1C"), 
                       location_data$PriceBin)



leaflet(location_data) %>% addProviderTiles("Esri.OceanBasemap") %>%
  addCircles(lng = ~long, lat = ~lat, 
             color = ~factpal(PriceBin))  %>%
  # controls
  setView(lng=center_lon, lat=center_lat,zoom = 12) %>%
  
  addLegend("bottomright", pal = factpal, values = ~PriceBin,
            title = "House Price Distribution",
            opacity = 1)

```

#### 1.4.6 Correlation analysis
We can observe that sqft_living and sqft_lot are highly correlated with sqft_living15 and sqft_lot15. Apparently these values are related to updates and renovations in 2015. Since houseage was created based on the building year there are highly negative correlated. Thus it makes sense only to include on for the model since both represent the same thing.

There are some variables with very high correlation to the price:

* sqft_living = 0.7
* grade = 0.67
* sqft_above = 0.61 (however highly correlated with sqft_living = 0.88)
* sqft_living15 = 0.59 (however highly correlated with sqft_living = 0.76)

We will need to take a closer look at the residuals of our model later on in order to check the effect of multicoliniarity and then decide if it is necessary to further exclude high correlated features.

```{r}
corr_data <- as.data.frame(prep_train_data)
corr_data <- corr_data[ , !(names(corr_data) %in% c('id', 'date', 'zipcode'))]
CorrelationResults = cor(corr_data)
#corrplot(CorrelationResults, type = 'lower')

corrplot(CorrelationResults, method = "color", outline = T, cl.pos = 'n', rect.col = "black",  tl.col = "indianred4", addCoef.col = "black", number.digits = 2, number.cex = 0.60, tl.cex = 0.7, cl.cex = 1, col = colorRampPalette(c("green4","white","red"))(100))

```

As we said before it might be that several of our discrete variables do not show a linear relationship to the price and thus should be turned into factore. Thus the relationship of the discrete as well as the continous variables is checked in the following.

##### Discrete variable relationship to the price
```{r}
options(repr.plot.width = 14, repr.plot.height = 14)

corr_data2 <- data.frame(prep_train_data)

corr_disc <- ggpairs(corr_data2[,c("price", discVar)]) +
  theme_minimal() +
  theme(text = element_text(face = "bold"),
        axis.text.x = element_text(angle = 90),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
corr_disc
```


##### Continous variable relationship to the price
```{r}
options(repr.plot.width = 14, repr.plot.height = 14)

corr_cont <- ggpairs(corr_data2[,c("price", contVar)]) +
  theme_minimal() +
  theme(text = element_text(face = "bold"),
        axis.text.x = element_text(angle = 90),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
corr_cont
```

### 1.5 Data cleaning
We have seen before that there were several data points with unusual data. For example, the house with 33 bedrooms. It is the only one that has more than 15 bedrooms and since it has only one floor it is highly unlikely that 33 bedrooms is actually correct. Especially for such a low price. Thus this row is excluded from the dataset. Eventhough it seemed in the histograms that there are houses with 0 bathrooms. However, there is none with 0 bathrooms but several with less than 1 (e.g. 0.5 or 0.75). This could actually be based on the limited things within a bathroom (e.g. no shower). These values might be correct and thus the rows are not excluded.

```{r}
show <- prep_train_data[which(prep_train_data$bedrooms > 15),]
datatable(show)
prep_train_data <- prep_train_data[-which(prep_train_data$bedrooms == 33),]

show<- prep_train_data[which(prep_train_data$bathrooms < 1),]
datatable(show)
```

---
title: "House Price Predictions - Predicitive Model (Part2)"
author: "Christine Utendorf"
date: "26 May 2019"
output: html_document
---

This project can be found within the following GitHub repository [link](https://github.com/cutendorf/House-Prices-R).

## 0. Sources and data loading

#### Notebook loading
Besides several libraries used for data exploration and visualization, the library loding also includes MASS, caret and ranger for the machine learning models.

All self-programed functions such as transforming or splitting a dataset are saved within the R notbook called functions.

Furthermore, several metrics that are not default for machine learning R packages are created as functions within the R notebook metrics. 
```{r source loading, warning=FALSE, message=FALSE}
source('notebooks/libraries.R')
source('notebooks/functions.R')
source('notebooks/metrics.R')
```

The work is divided into two R markdowns. The first one only focusses on data exploration and preparation and the second one on modeling. This is due to the fact that each markdown takes some while to be created and with this division the whole data exploration does not need to be computed again while modeling.

#### Data loading
From the data preparation and exploration part the prepared train and test data sets are loaded.

```{r data loading}
train_data<-fread('data/data_train_dummy.csv', stringsAsFactors = F)
test_data<-fread('data/data_test_dummy.csv', stringsAsFactors = F)

train_data$date<-as.Date(train_data$date)
test_data$date<-as.Date(test_data$date)
```

#### Splitting dataset
Since our test set is blind and we want to compare models an select the best one, a list object is created that not only stores the blind test data set but also a train data set (80% of train data) and a holdout set (20% of train data) that was created from the overall provided train data. In order to evaluate and monitor the performance of the models also k-folds cross-validation is performed on the 80% train data set. Furthermore the predictions on the holdout data set are evaluated in order to see how well the model performs on data that is has nevern been trained on before. Crossvalidation results might already be enough in this case to evaluate but since we also want to track non-default measures such as MAPE it is easier to calculated them on a holdout prediction set.

Since we want to predict the price, which is our target variable, this is stored in a variable called formular that uses price as target and all other variables of the data set as features, which are all turned into numeric variables. 


We store the id and the price data for the holdout in a seperate data set in order to calculate later the metrics. Furthermore also the ids of the blind test set are stored in a seperate dataset to later on match the final predictions for submission.
(Even though I could exclude date from my model I had trouble when making predictions for the holdout, thus for the baseline models I exclude date from the data set and then when it come to feature engineering I include it again if any features are constructed based on date)

Id is then excluded from all datasets since there were severe problems when trying to predict with the models and not using the id as an input.

Before starting to model an empty data frame is created to store the baseline results.

```{r data set up}
whole_data = f_partition(train_data, test_data, test_proportion=0.2, seed = 1)
whole_data<-lapply(whole_data, function(x){
  return(x[, which(sapply(x, is.integer)):=lapply(.SD, as.numeric), .SDcols=sapply(x,is.integer)])
})

#df_test<-whole_data$holdout[,c("id", "price")]
df_test<-whole_data$holdout[, .(id=1:.N, price)]
df_pred<-whole_data$test[,"id"]

whole_data$train$id = NULL
whole_data$test$id = NULL
whole_data$holdout$id = NULL

# Having trouble excluding date column from lm model thus I save it in seperate datasets for later feature engineering
train_date<-as.data.frame(whole_data$train$date)
test_date<-as.data.frame(whole_data$test$date)
holdout_date<-as.data.frame(whole_data$holdout$date)

whole_data$train$date = NULL
whole_data$test$date = NULL
whole_data$holdout$date = NULL


formula<-as.formula(price~.)

result<-data.table(method=c('lm','glmnet','rf','xgb'),R2cv=0, RMSE=0, MAE=0,MAPE=0)
```


## 1. Baseline models
In order to select an algorithm for this predicitve model, all algorithms are trained on the train set and then based on their CV scores and test data scores the best model is chosen. Since no feature engineering or feature selection has been performed until now, all these models can be seen as baselines. The best baselinemodel is then chosen and from there on feature engineering, feature selection and hyperparameter tuning is performed. This approach is mainly chosen due to time and computing power constraints. For example, random forest and xgboost takes quiet some time not only to train but then later on to tune with for example grid search.

### 1.1 Linear regression model
Since we are predicting a continous target, it makes sense to start with a linear regression. Eventhough we have many features now due to dummy encoding, linear regression gives a good starting point for the modeling.

#### 1.1.1 Trained linear model
The trained linear model results in an adjusted R2 of 0.815 with many significant variables. However, several features show up as NAs. The main reason for this is that due to the dummy encoding with full rank. For example view had values from 0 to 4. Thus if all values for view 0 to 3 are False view 4 has to be True. Thus we see here a perfect connection between these variables since we do not need the last one because its explained by the combination of the others. When we use a step-wise linear regression these NA values will be excluded right away from the model. However, for computing time reasons we stick with a normal linear model since a linear model with stepwise feature selection takes mch longer to train.

```{r linear model, warning=FALSE, message=FALSE}
lm <- lm(formula = formula, 
                 data=whole_data$train)
summary(lm)

## Example for LM stepwise feature selection
# lm_step <-stepAIC(lm(formula = formula, data=whole_data$train), trace=F)
# summary(lm_step)
```

#### 1.1.2 Cross validation

In order to further evaluate the model a k-fold crossvaldiation is performed with 5 folds. The R2 for the cross valdiation is 0.812 which is quiet close to our actual R2 from the trained model and thus the model is not tending to overfit. The R2 result is saved in the result dataframe.

```{r linear model CV, warning=FALSE, message=FALSE}
set.seed(123) 
train.control <- trainControl(method = "cv", number = 5)


lm_cv <- train(formula, data = whole_data$train, method = "lm",
               trControl = train.control)

print(lm_cv)

#R2cv_lm<-as.numeric(lm_cv$results$Rsquared)
result[1,2]<-round(lm_cv$results$Rsquared, digits=4)
```

#### 1.1.3 Test performance 

The next step is to check how good the model prediciton for the holdout are. Overall we see that for the lower prices houses the models seems to be working well. Howver, to predict the prices for more expensive houses the model tends to underestimate the price.

```{r linear model test, warning=FALSE, message=FALSE}
test_lm<-predict(lm, newdata = whole_data$holdout)

df_test<-cbind(df_test, test_lm)


ggplot(df_test, aes(x=price,y=test_lm))+
      geom_point(alpha=0.5)+xlab('actual')+ylab('predictions')+
      #+ylim(0,2000000)+xlim(0,2000000)
      geom_abline(slope=1, intercept=0)+
      ggtitle('Linear Regression - Test prediction')
```


#### 1.1.4 Residuals

Looking at the residuals supports our findings from the previous two plots. Thus this model would definetly need further improvement.

```{r linear model test residuals}
residualPlot(lm, ask=F)
```

#### 1.1.5 Result overview

Finally the results are calculated and saved in the result dataframe. The MAPE (our main perfomance measure in this case) lies at 0.19.

```{r linear model results}
result[1,3]<-round(rmse(real=whole_data$holdout$price, predicted = test_lm), digits=0)
result[1,4]<-round(mae(real=whole_data$holdout$price, predicted = test_lm), digits=0)
result[1,5]<-round(mape(real=whole_data$holdout$price, predicted = test_lm), digits=4)
datatable(result)
```


### 1.2 Linear regression model with regularization (Lasso)
The next model in line is a linear regression model with regularization. In this case we use a lasso regression which is a shrinkage and variable selection method for linear regressions. The objective of this method is to obtain a subset of features that minimizes the prediction error for the target variable.

#### 1.2.1 Trained linear model with regularization
The first step is to find lambda, which is a numeric valuedefining the amount of shrinkage. The best value for lambda is the one that minimzes the cv-preciction error rate. To do so cv.glmnet is used from the glmnet library.

```{r glmnet cv, warning=FALSE, message=FALSE}
glmnet_cv<-cv.glmnet(x = data.matrix(whole_data$train[, !'price']),
                     nfolds = 5,
                     y = whole_data$train[['price']],
                     alpha=1,
                     family = 'gaussian',
                     standardize = T)
plot.cv.glmnet(glmnet_cv)

glmnet_cv$lambda.min
```

In order to compute now the lasso regression lambda.min is used as shrinkage parameter. We obtain a R2 score of 0.817 which is slightly better than the one for the simple linear regression.

```{r glmnet , warning=FALSE, message=FALSE}
glmnet_0<-glmnet(x = data.matrix(whole_data$train[, !'price']), 
                 y = whole_data$train[['price']],
                 family = 'gaussian',
                 alpha=1, lambda = glmnet_cv$lambda.min)

glmnet_0
glmnet_0$beta
result[2,2]<-round(glmnet_0$dev.ratio, digits=4)
```

#### 1.2.2 Cross validation

There is no cross validation performed here since the minimum lambda was already obtain by using cross-validation.

#### 1.2.3 Test performance 

The next step is to make predictions for our holdout data set. Here we see a similar prediction behavior to the former model. For lower priced houses the model predict much better than for more expensive ones (+2 mio).

```{r glmnet test, warning=FALSE, message=FALSE}
test_glmnet<-predict(glmnet_0, newx = as.matrix(whole_data$holdout[, !'price']))


df_test<-cbind(df_test, test_glmnet=test_glmnet[,1])



ggplot(df_test, aes(x=price,y=test_glmnet))+
      geom_point(alpha=0.5)+xlab('actual')+ylab('predictions')+
      #+ylim(0,2000000)+xlim(0,2000000)
      geom_abline(slope=1, intercept=0)+
      ggtitle('Lasso Regression - Test prediction')
```

#### 1.2.4 Residuals

Looking at the residuals supports our findings from the previous two plots eventhough it seems like the residuals are overall closer to zero than in the linear model. Nevertheless this model would definetly need further improvement.
To plot the residuals the plotmo library was used. 

```{r glmnet test residuals}
plotres(glmnet_0, which=3)
```

#### 1.2.5 Result overview

Finally the results are calculated and saved in the result dataframe. The MAPE (our main perfomance measure in this case) lies around 0.19 as well - slightly below (=better) the result of the simple linear regression.

```{r glmnet results}
result[2,3]<-round(rmse(real=whole_data$holdout$price, predicted = test_glmnet), digits=0)
result[2,4]<-round(mae(real=whole_data$holdout$price, predicted = test_glmnet), digits=0)
result[2,5]<-round(mape(real=whole_data$holdout$price, predicted = test_glmnet), digits=4)
datatable(result)
```


### 1.3 Random forest model
The next model in line is random forest. In order to work with random forest, the ranger library is used. Random forest operates by constructing many decision trees and then taking the mean prediciton of all individual trees.


#### 1.3.1 Trained random forest model
The trained random forest model constructed 500 trees and used 105 independent variables. The R2 score lies at 0.854 which is higher than the two previous models.

```{r random forest, warning=FALSE, message=FALSE}
rf<-ranger(formula, whole_data$train)

summary(rf)

result[3,2]<-round(rf$r.squared, digits=4)
```

#### 1.3.2 Cross validation

Due to time constraints no cross validation is applied for this model at this point.

```{r random forest CV, warning=FALSE, message=FALSE}
#set.seed(123) 
#rf_cv <- rgcv(whole_data$train[, !'price'], whole_data$train$price, cv.fold = 5)
#rf_cv1
```

#### 1.3.3 Test performance 

The next step is to check how good the model prediciton for the holdout are. Overall we see that the predicted values and actual values are closer together than for the previous two models. However, the model has still problems to predict expensive houses accurately.

```{r random forest test, warning=FALSE, message=FALSE}
test_rf<-predict(rf,whole_data$holdout)$predictions

df_test<-cbind(df_test, test_rf)


ggplot(df_test, aes(x=price,y=test_rf))+
      geom_point(alpha=0.5)+xlab('actual')+ylab('predictions')+
      #+ylim(0,2000000)+xlim(0,2000000)
      geom_abline(slope=1, intercept=0)+
      ggtitle('Random Forest - Test prediction')
```


#### 1.3.4 Residuals

Also by looking at the residuals we see again that the model performs better in the lower price region than in the high price region.

```{r random forest test residuals}
rf_residuals <- whole_data$train$price - rf$predictions

residual_plot<-as.data.frame(whole_data$train$price)
residual_plot<-cbind(residual_plot,rf_residuals)

ggplot(residual_plot, aes(x=whole_data$train$price, rf_residuals)) +
    geom_point(shape=1) +
    geom_smooth()+
    theme_minimal()+
    ylab('Residuals')+
    xlab('Fitted values')
```

#### 1.3.5 Result overview

Finally the results are calculated and saved in the result dataframe. The MAPE (our main perfomance measure in this case) lies at 0.14, which is significantly lower than the two linear models. Also by looking at the other three performance measures we see that the random forest model outperforms the other two models.

```{r random forest results}
result[3,3]<-round(rmse(real=whole_data$holdout$price, predicted = test_rf), digits=0)
result[3,4]<-round(mae(real=whole_data$holdout$price, predicted = test_rf), digits=0)
result[3,5]<-round(mape(real=whole_data$holdout$price, predicted = test_rf), digits=4)
datatable(result)
```


### 1.4 Boosting trees model
The last baseline model is a tree based xgboost model. 


#### 1.4.1 Trained random forest model
The model uses cross validation with 5 folds. Since R2 is not a default parameter for xgboost but only RMSE it was not possible to construct it from the model. I tried to access the training predictions of the model but I did not manage to find it. We can not use the model to predict on the train data and then use these predicitons because the model was trained on it. The R2 result would lie around 0.96 (I tried it). Thus for now the R2 will remain empty.

```{r xgboost, warning=FALSE, message=FALSE, results=FALSE}
xgb<-xgboost(booster='gbtree',
               data=as.matrix(whole_data$train[, !'price', with=F]),
               label=whole_data$train$price,
               nrounds = 100,
               objective='reg:linear')

```

```{r xgboost train results}
print(xgb)

#training_predictions = ???
#res <- caret::postResample(whole_data$train$price, training_predictions)
#rsq <- res[2]
#result[4,2]<-round(rsq, digits=4)
result[4,2]<-NA
```

#### 1.4.2 Cross validation

Xgboost is a powerful algorithm but brings in the thread of overfitting. We already see by looking at the RMSE that the score is much higher for train than test data. Nevertheless the RMSE score in the last iteration is lower than the test scores for the other three models (lm, glmnet, rf).
```{r cross validation xgboost}
xgb_cv <- xgb.cv(objective='reg:linear',
                  data=as.matrix(whole_data$train[, !'price', with=F]),
                  label=whole_data$train$price,
                  nrounds = 100, 
                  nfold = 5)
```


#### 1.4.3 Test performance 

The next step is to check how good the model prediciton for the holdout are. It seems that the model slightly improved here but still the problem for expensive houses remains.

```{r xgboost test, warning=FALSE, message=FALSE}
test_xgb<-predict(xgb, newdata = as.matrix(whole_data$holdout[, !'price', with=F]), type='response')


df_test<-cbind(df_test, test_xgb)


ggplot(df_test, aes(x=price,y=test_xgb))+
      geom_point(alpha=0.5)+xlab('actual')+ylab('predictions')+
      #+ylim(0,2000000)+xlim(0,2000000)
      geom_abline(slope=1, intercept=0)+
      ggtitle('XGBoost - Test prediction')
```


#### 1.4.4 Residuals

Since I cannot access the training predicitons it is not possible to draw a residual plot for the training model.

#### 1.4.5 Result overview

Finally the results are calculated and saved in the result dataframe. The MAPE (our main perfomance measure in this case) lies at 0.1276, which is even lower than the random forest model. Also for the two other parameter the results of the xgboost model are better. We actually see that the test RSME score is above out test RSME score from the cross validation. This shows us that the model is tending to overfit.

```{r xgboost results}
result[4,3]<-round(rmse(real=whole_data$holdout$price, predicted = test_xgb), digits=0)
result[4,4]<-round(mae(real=whole_data$holdout$price, predicted = test_xgb), digits=0)
result[4,5]<-round(mape(real=whole_data$holdout$price, predicted = test_xgb), digits=4)
datatable(result)
```

### 1.5 Baseline conclusion

Comparing the four models we see that the best baseline is achieved by using an boosting tree model (xgboost). Thus for feature engineering all features will be evaluated by using the xgboost model. In a final step hyperparameter tuning will be performed for xgboost.


## 2. Feature engineering

In the next step new features are created and the models performance is evaluated for each feature. All features that improve the baseline will be included in the final data set. Thus first of all a copy of our train and holdout data set is created as well as a result table that stores the performance measures for each feature engineering steps.

```{r data set up fe}
fe_results<-as.data.frame(result[4,c(1,3:5)])

train_fe<-whole_data$train
test_fe<-whole_data$holdout

df_test_fe<-as.data.frame(test_fe$price)
names(df_test_fe)[1]<-paste("price")
```

All new features are created with functions which makes it easier to apply them to all the data sets. The functions are saved in a separate notebook and loaded in the next step.

```{r source loading fe}
source('notebooks/feature_engineering.R')
```

### 2.1 Basement
Since many of the houses have a basement size of zero, we can assume that all these houses do not have a zero. Thus a possibility is to replace the numerical variable sqft_basement with a boolean variable that indicates if the house has a basement or not. From a logical point of view it does not seem too important how big a basement is but rather if the house has a basement at all. Thus a new feature is created called basement and the old feature sqft_basement is excluded.

```{r data set up basement}
train_fe<-basement(train_fe)
test_fe<-basement(test_fe)
```

The xgboost model in now trained with the new boolean variable basement but without the old variable sqft_basement.
```{r basement feature, results=FALSE}
xgb_basement<-xgboost(booster='gbtree',
               data=as.matrix(train_fe[, !'price', with=F]),
               label=train_fe$price,
               nrounds = 100,
               objective='reg:linear')

test_xgb_basement<-predict(xgb_basement, newdata = as.matrix(test_fe[, !'price', with=F]), type='response')


df_test_fe<-cbind(df_test_fe, test_xgb_basement)

fe_results[2,1]<-"basement"
fe_results[2,2]<-round(rmse(real=df_test_fe$price, predicted = df_test_fe$test_xgb_basement), digits=0)
fe_results[2,3]<-round(mae(real=df_test_fe$price, predicted = test_xgb_basement), digits=0)
fe_results[2,4]<-round(mape(real=df_test_fe$price, predicted = test_xgb_basement), digits=4)
```

```{r result basement}
datatable(fe_results)

```
We see that the with including the boolean basement feature the RMSE and the MAE slightly improved while the MAPE actually worsened and is now above 0.13. Since the MAPE is our preferred metric for this case the basement variable will not be included.

### 2.2 Rennovation
Similiar to the basement feature, there are many houses that show a 0 for their renovation year meaning they have never been rennovated. However, for renovations time actually matters. We can assume that a renovation during the last ten years is more valuable than one that was 20 years ago. Thus a new boolean feature is created that flaggs all houses that were rennovated during the last ten years (since 2005).

```{r data set up rennovated}
train_fe<-whole_data$train
test_fe<-whole_data$holdout

train_fe<-renovated(train_fe)
test_fe<-renovated(test_fe)
```

The xgboost model in now trained with the new boolean variable renovated but without the old variable yr_renovated.
```{r rennovation feature, results=FALSE}
xgb_renovated<-xgboost(booster='gbtree',
               data=as.matrix(train_fe[, !'price', with=F]),
               label=train_fe$price,
               nrounds = 100,
               objective='reg:linear')

test_xgb_renovated<-predict(xgb_renovated, newdata = as.matrix(test_fe[, !'price', with=F]), type='response')


df_test_fe<-cbind(df_test_fe, test_xgb_renovated)

fe_results[3,1]<-"renovated"
fe_results[3,2]<-round(rmse(real=df_test_fe$price, predicted = df_test_fe$test_xgb_renovated), digits=0)
fe_results[3,3]<-round(mae(real=df_test_fe$price, predicted = df_test_fe$test_xgb_renovated), digits=0)
fe_results[3,4]<-round(mape(real=df_test_fe$price, predicted = df_test_fe$test_xgb_renovated), digits=4)
```

```{r result rennovated}
datatable(fe_results)
```
This feature shows worse values for all three performance measures and thus is not included in the model.

### 2.3 Viewing
In the data exploration the feature view showed a high skewness. Most houeses had not been viewed before the sale leaving many houses with zero views. Thus a possibility is to create a feature that shows if a house has been viewed or not and with this combining all the views from 1 to 4. A way to create such a boolean feature is to only include the encoded feature view_0 and excluding the feature view_1 to view_4.

```{r data set up viewed}
train_fe<-whole_data$train
test_fe<-whole_data$holdout

train_fe<-viewed(train_fe)
test_fe<-viewed(test_fe)
```

The xgboost model in now trained excluding the variables view_1 to view_4
```{r viewed feature, results=FALSE}
xgb_viewed<-xgboost(booster='gbtree',
               data=as.matrix(train_fe[, !'price', with=F]),
               label=train_fe$price,
               nrounds = 100,
               objective='reg:linear')

test_xgb_viewed<-predict(xgb_viewed, newdata = as.matrix(test_fe[, !'price', with=F]), type='response')


df_test_fe<-cbind(df_test_fe, test_xgb_viewed)

fe_results[4,1]<-"viewed"
fe_results[4,2]<-round(rmse(real=df_test_fe$price, predicted = df_test_fe$test_xgb_viewed), digits=0)
fe_results[4,3]<-round(mae(real=df_test_fe$price, predicted = df_test_fe$test_xgb_viewed), digits=0)
fe_results[4,4]<-round(mape(real=df_test_fe$price, predicted = df_test_fe$test_xgb_viewed), digits=4)
```

```{r result viewed}
datatable(fe_results)

```
Also this step worsens all three performance scores compared to the baseline.

### 2.4 Sqft living comparison
The variable sqft_living15 indicates the living size of the nearest 15 houses and thus is a good benchmark for the area around the actual house. Is the living size above this value then the house is comparably larger than the ones around and the other way around. Thus a new feature is created calculating the ratio between the living size of the house and its 15 nearest neighbors. A value below 1 means that the house is on average smaller compared to its neighbors while a value above 1 means is on average lager.

```{r data set up living comparison}
train_fe<-whole_data$train
test_fe<-whole_data$holdout

train_fe<-living_comp(train_fe)
test_fe<-living_comp(test_fe)
```

The xgboost model in now trained inclduing the new ratio variable and excluding the variables sqft_living15.
```{r sqft living comparison feature, results=FALSE}
xgb_livcomp<-xgboost(booster='gbtree',
               data=as.matrix(train_fe[, !'price', with=F]),
               label=train_fe$price,
               nrounds = 100,
               objective='reg:linear')

test_xgb_livcomp<-predict(xgb_livcomp, newdata = as.matrix(test_fe[, !'price', with=F]), type='response')


df_test_fe<-cbind(df_test_fe, test_xgb_livcomp)

fe_results[5,1]<-"living_comp"
fe_results[5,2]<-round(rmse(real=df_test_fe$price, predicted = df_test_fe$test_xgb_livcomp), digits=0)
fe_results[5,3]<-round(mae(real=df_test_fe$price, predicted = df_test_fe$test_xgb_livcomp), digits=0)
fe_results[5,4]<-round(mape(real=df_test_fe$price, predicted = df_test_fe$test_xgb_livcomp), digits=4)
```

```{r result living comparison}
datatable(fe_results)

```
Also this feature does not improve the performance measures and is thus not included in the model.

### 2.5 Sqft lot comparison
A similiar feature for the lot size is created as the one for living size in 2.4.

```{r data set up lot comparison}
train_fe<-whole_data$train
test_fe<-whole_data$holdout

train_fe<-lot_comp(train_fe)
test_fe<-lot_comp(test_fe)
```

The xgboost model in now trained inclduing the new ratio variable and excluding the variables sqft_lot15.
```{r sqft lot comparison feature, results=FALSE}
xgb_lotcomp<-xgboost(booster='gbtree',
               data=as.matrix(train_fe[, !'price', with=F]),
               label=train_fe$price,
               nrounds = 100,
               objective='reg:linear')

test_xgb_lotcomp<-predict(xgb_lotcomp, newdata = as.matrix(test_fe[, !'price', with=F]), type='response')


df_test_fe<-cbind(df_test_fe, test_xgb_lotcomp)

fe_results[6,1]<-"lot_comp"
fe_results[6,2]<-round(rmse(real=df_test_fe$price, predicted = df_test_fe$test_xgb_lotcomp), digits=0)
fe_results[6,3]<-round(mae(real=df_test_fe$price, predicted = df_test_fe$test_xgb_lotcomp), digits=0)
fe_results[6,4]<-round(mape(real=df_test_fe$price, predicted = df_test_fe$test_xgb_lotcomp), digits=4)
```

```{r result lot comparison}
datatable(fe_results)

```
Also this feature does not improve the performance measures and is thus not included in the model.

### 2.6 Decade of building year
As we have seen during the data exploration, the average prices of houses tended to drop depending if the house was build during mid 20th century. Thus instead of the building year a new feature is included that states the decade in which the house was built.

```{r data set up decade}
train_fe<-whole_data$train
test_fe<-whole_data$holdout

train_fe<-decade(train_fe)
test_fe<-decade(test_fe)
```

The xgboost model in now trained inclduing the decade variable and excluding the variables yr_built.
```{r decade feature, results=FALSE}
xgb_decade<-xgboost(booster='gbtree',
               data=as.matrix(train_fe[, !'price', with=F]),
               label=train_fe$price,
               nrounds = 100,
               objective='reg:linear')

test_xgb_decade<-predict(xgb_decade, newdata = as.matrix(test_fe[, !'price', with=F]), type='response')


df_test_fe<-cbind(df_test_fe, test_xgb_decade)

fe_results[7,1]<-"decade"
fe_results[7,2]<-round(rmse(real=df_test_fe$price, predicted = df_test_fe$test_xgb_decade), digits=0)
fe_results[7,3]<-round(mae(real=df_test_fe$price, predicted = df_test_fe$test_xgb_decade), digits=0)
fe_results[7,4]<-round(mape(real=df_test_fe$price, predicted = df_test_fe$test_xgb_decade), digits=4)
```

```{r result decade}
datatable(fe_results)
```
Also this feature does not improve the performance measures and is thus not included in the model.

### 2.7 Low correlation features
As we have seen in the correlation plot during the data exploration there are several features with low correlation to the target:
* condition: corr = 0.04
* sqft_lot15 : cor = 0.08

Even though this cannot really be regarded as feature engineering but rather feature selection for the purpose of easy comparison this step is inlcuded under this section.

```{r data set up low corr}
train_fe<-whole_data$train
test_fe<-whole_data$holdout

train_fe<-low_correlation(train_fe)
test_fe<-low_correlation(test_fe)
```

The xgboost model in now trained inclduing the decade variable and excluding the variables yr_built.
```{r low correlation feature, results=FALSE}
xgb_lowcorr<-xgboost(booster='gbtree',
               data=as.matrix(train_fe[, !'price', with=F]),
               label=train_fe$price,
               nrounds = 100,
               objective='reg:linear')

test_xgb_lowcorr<-predict(xgb_lowcorr, newdata = as.matrix(test_fe[, !'price', with=F]), type='response')


df_test_fe<-cbind(df_test_fe, test_xgb_lowcorr)

fe_results[8,1]<-"low_correlation"
fe_results[8,2]<-round(rmse(real=df_test_fe$price, predicted = df_test_fe$test_xgb_lowcorr), digits=0)
fe_results[8,3]<-round(mae(real=df_test_fe$price, predicted = df_test_fe$test_xgb_lowcorr), digits=0)
fe_results[8,4]<-round(mape(real=df_test_fe$price, predicted = df_test_fe$test_xgb_lowcorr), digits=4)
```

```{r result decade}
datatable(fe_results)
```
Excluding low correlated features worsened the performance of our model. Thus all initial features are kept.

### 2.8 Feature engineering conclusion
As seen in the feature engineering result table above, none of the feature engineering steps helped to improve our model. Thus we will continue with out initial dataset.

## 3. Hyperparameter tuning
The last step before training the final model with all the labeled data is to do hyperparameter tuning. First of all we set up all needed data sets and results dataframes to start with the tuning.

```{r data set up hyperparameter tuning}
train_hyp<-whole_data$train
test_hyp<-whole_data$holdout

df_test_hyp<-as.data.frame(df_test_fe[,1])
names(df_test_hyp)[1]<-paste("price")

hyp_results<-fe_results[1,]
```

When we use an xgboost algorithm there are several hyperparameter that can be changed:
*eta = learning rate [0 < eta < 1]
*min_child_weight = minimum sum of weights of all observations required in a "child" - higher values prevent over-fitting, too hogh values have risk of under-fitting
*max_depth = maximum depth of a tree - high depth can lead to overfitting
*colsample_bytree = subsample ratio of columns when constructing each tree
*gamma = specifies the minimum loss reduction required to make a split
*subsample = denotes the fraction of observations to be randomly samples for each tree - lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting.

Except for subsample each of the other parameters has minimum two different values within the tuning grid. For each parameter the default value is included within the tuning grid. The tuning grid includes all possible combinations of the different parameter values.

The best tune is then obtained by fitting the xgboost model with all different parameter combination of the tune grid using a 5 k-fold cross validation. The metric used to tune is the RSME due to the fact that MAPE is not a default metric for the hyperparameter tuning using caret.

```{r hyperparameter tuning, results=FALSE}
# 1. Define our grid of hyperparameters to tune
tuneGrid <- expand.grid(nrounds = 100,
                        eta = c(0.1,0.2,0.3),
                        min_child_weight=c(1,3,5),
                        max_depth = c(6,8,10),
                        colsample_bytree = c(0.5, 0.8, 1),
                        gamma = c(0,0.1),
                        subsample=1)


# 2. Define the validation squema
ctrl <- trainControl(
  method = "cv",
  number = 5,
  savePredictions=TRUE
)

# 3. Train the model
ini<-now()
set.seed(123)
xgboostFit <- train(
      formula,
      data = train_hyp,
      method = "xgbTree",
      preProc = NULL, 
      tuneGrid = tuneGrid,
      trControl = ctrl,
      metric = "RMSE"
)
print(now()-ini)

# we save the train object
saveRDS(xgboostFit,'data/xgboostFit_auto.RData')
```

The hyperparameter model is then saved and all the parameters from the best tune are used for the xgboost model. By looking at the prediction plot we still see that the model underperforms for more expensive houses.

```{r hyperparameter model}
params_hyp = list(eta=xgboostFit$bestTune$eta, max_depth=xgboostFit$bestTune$max_depth,gamma = xgboostFit$bestTune$gamma,subsample=1,colsample_bytree=xgboostFit$bestTune$colsample_bytree,min_child_weight=xgboostFit$bestTune$min_child_weight)

xgb_hyp<-xgboost(booster='gbtree',
                     data=as.matrix(train_hyp[, !'price', with=F]),
                     label=train_hyp$price,
                     nrounds = 100,
                     params = params_hyp,
                     objective='reg:linear'
                    )

test_xgb_hyp<-predict(xgb_hyp, newdata = as.matrix(test_hyp[, !'price', with=F]), type='response')

df_test_hyp<-cbind(df_test_hyp, test_xgb_hyp)

ggplot(df_test_hyp, aes(x=price,y=test_xgb_hyp))+
      geom_point(alpha=0.5)+xlab('actual')+ylab('predictions')+
      #+ylim(0,2000000)+xlim(0,2000000)
      geom_abline(slope=1, intercept=0)+
      ggtitle('XGBoost tuned - Test prediction')
```

```{r hyperparameter results}
hyp_results[2,1]<-"hyp_tuning"
hyp_results[2,2]<-round(rmse(real=df_test_hyp$price, predicted = df_test_hyp$test_xgb_hyp), digits=0)
hyp_results[2,3]<-round(mae(real=df_test_hyp$price, predicted = df_test_hyp$test_xgb_hyp), digits=0)
hyp_results[2,4]<-round(mape(real=df_test_hyp$price, predicted = df_test_hyp$test_xgb_hyp), digits=4)

datatable(hyp_results)
```
Overall, we see that the hyperparameter tuning did not help us improve our model. Thus we will continue and construct our final model with the default xgboost model.

## 4. Final model
In order to construct the final model for our predicitons the model is retrained by using all the labeled test data. Thus the train and the holdout data set are combined again and then used to train the final xgboost model.

```{r data set up final model}
final_train_data <- rbind(whole_data$train, whole_data$holdout)
final_test_data <- whole_data$test
```

The final xgboost model is trained with the complete labeled data set.
```{r final xgboost, warning=FALSE, message=FALSE, results=FALSE}
xgb_final<-xgboost(booster='gbtree',
               data=as.matrix(final_train_data[, !'price', with=F]),
               label=final_train_data$price,
               nrounds = 100,
               objective='reg:linear')

```

In order to monitor overfitting cross validation with 5 folds is performed. The test score lies above the train score which shows a sign of overfitting. Nevertheless such a behavior is normal and since the test score remains constant around an RMSE of 127,556 we can continue with this model.
```{r cross validation final xgboost}
xgb_final_cv <- xgb.cv(objective='reg:linear',
                  data=as.matrix(final_train_data[, !'price', with=F]),
                  label=final_train_data$price,
                  nrounds = 100, 
                  nfold = 5)
```

The model is now used the obtain the predictions for the blind test data set. The predictions are then saved together with the previous used house sale id in a csv file that can be found in the data folder.
```{r final xgboost predictions}
xgb_final_pred<-predict(xgb_final, newdata = as.matrix(final_test_data), type='response')

predictions<-cbind(df_pred,xgb_final_pred)
write.csv(predictions,'data/predictions.csv', row.names = FALSE)
```

## 5. Final remark
I hope you enjoyed my project. There is always more to add, to tune, to try. However, I am quiete happy with the result and the fact that I accomplished to cover every step of the machine learning process (including hyperparameter tuning which I never really touched before). Now I hope my predicitons on the blind test data set are not too bad ;)
---
title: "House Price Predictions - Predicitive Model (Part2)"
author: "Christine Utendorf"
date: "26 May 2019"
output: html_document
---

## 0. Sources and data loading

#### Notebook loading
Besides several libraries used for data exploration and visualization, the library loding also includes MASS, caret and ranger for the machine learning models.

All self-programed functions such as transforming or splitting a dataset are saved within the R notbook called functions.

Furthermore, several metrics that are not default for machine learning R packages are created as functions within the R notebook metrics. 
```{r source loading, warning=FALSE, message=FALSE}
source('notebooks/libraries.R')
source('notebooks/functions.R')
source('notebooks/metrics.R')
```

The work is divided into two R markdowns. The first one only focusses on data exploration and preparation and the second one on modeling. This is due to the fact that each markdown takes some while to be created and with this division the whole data exploration does not need to be computed again while modeling.

#### Data loading
From the data preparation and exploration part the prepared train and test data sets are loaded.

```{r data loading}
train_data<-fread('data/data_train_dummy.csv', stringsAsFactors = F)
test_data<-fread('data/data_test_dummy.csv', stringsAsFactors = F)

train_data$date<-as.Date(train_data$date, "%m/%d/%Y")
test_data$date<-as.Date(test_data$date, "%m/%d/%Y")
```

#### Splitting dataset
Since our test set is blind and we want to compare models an select the best one, a list object is created that not only stores the blind test data set but also a train data set (80% of train data) and a holdout set (20% of train data) that was created from the overall provided train data. In order to evaluate and monitor the performance of the models also k-folds cross-validation is performed on the 80% train data set. Furthermore the predictions on the holdout data set are evaluated in order to see how well the model performs on data that is has nevern been trained on before. Crossvalidation results might already be enough in this case to evaluate but since we also want to track non-default measures such as MAPE it is easier to calculated them on a holdout prediction set.

Since we want to predict the price, which is our target variable, this is stored in a variable called formular that uses price as target and all other variables of the data set as features, which are all turned into numeric variables. 


We store the id and the price data for the holdout in a seperate data set in order to calculate later the metrics. Furthermore also the ids of the blind test set are stored in a seperate dataset to later on match the final predictions for submission.
(Even though I could exclude date from my model I had trouble when making predictions for the holdout, thus for the baseline models I exclude date from the data set and then when it come to feature engineering I include it again if any features are constructed based on date)

Id is then excluded from all datasets since there were severe problems when trying to predict with the models and not using the id as an input.

Before starting to model an empty data frame is created to store the baseline results.

```{r data set up}
whole_data = f_partition(train_data, test_data, test_proportion=0.2, seed = 1)
whole_data<-lapply(whole_data, function(x){
  return(x[, which(sapply(x, is.integer)):=lapply(.SD, as.numeric), .SDcols=sapply(x,is.integer)])
})

#df_test<-whole_data$holdout[,c("id", "price")]
df_test<-whole_data$holdout[, .(id=1:.N, price)]
df_pred<-whole_data$test[,"id"]

whole_data$train$id = NULL
whole_data$test$id = NULL
whole_data$holdout$id = NULL

# Having trouble excluding date column from lm model thus I save it in seperate datasets for later feature engineering
train_date<-as.data.frame(whole_data$train$date)
test_date<-as.data.frame(whole_data$test$date)
holdout_date<-as.data.frame(whole_data$holdout$date)

whole_data$train$date = NULL
whole_data$test$date = NULL
whole_data$holdout$date = NULL


formula<-as.formula(price~.)

result<-data.table(method=c('lm','glmnet','rf','xgb'),R2cv=0, RMSE=0, MAE=0,MAPE=0)
```


## 1. Baseline models
In order to select an algorithm for this predicitve model, all algorithms are trained on the train set and then based on their CV scores and test data scores the best model is chosen. Since no feature engineering or feature selection has been performed until now, all these models can be seen as baselines. The best baselinemodel is then chosen and from there on feature engineering, feature selection and hyperparameter tuning is performed. This approach is mainly chosen due to time and computing power constraints. For example, random forest and xgboost takes quiet some time not only to train but then later on to tune with for example grid search.

### 1.1 Linear regression model
Since we are predicting a continous target, it makes sense to start with a linear regression. Eventhough we have many features now due to dummy encoding, linear regression gives a good starting point for the modeling.

#### 1.1.1 Trained linear model
The trained linear model results in an adjusted R2 of 0.815 with many significant variables. However, several features show up as NAs. The main reason for this is that due to the dummy encoding with full rank. For example view had values from 0 to 4. Thus if all values for view 0 to 3 are False view 4 has to be True. Thus we see here a perfect connection between these variables since we do not need the last one because its explained by the combination of the others. When we use a step-wise linear regression these NA values will be excluded right away from the model. However, for computing time reasons we stick with a normal linear model since a linear model with stepwise feature selection takes mch longer to train.

```{r linear model, warning=FALSE, message=FALSE}
lm <- lm(formula = formula, 
                 data=whole_data$train)
summary(lm)

## Example for LM stepwise feature selection
# lm_step <-stepAIC(lm(formula = formula, data=whole_data$train), trace=F)
# summary(lm_step)
```

#### 1.1.2 Cross validation

In order to further evaluate the model a k-fold crossvaldiation is performed with 5 folds. The R2 for the cross valdiation is 0.812 which is quiet close to our actual R2 from the trained model and thus the model is not tending to overfit. The R2 result is saved in the result dataframe.

```{r linear model CV, warning=FALSE, message=FALSE}
set.seed(123) 
train.control <- trainControl(method = "cv", number = 5)


lm_cv <- train(formula, data = whole_data$train, method = "lm",
               trControl = train.control)

print(lm_cv)

#R2cv_lm<-as.numeric(lm_cv$results$Rsquared)
result[1,2]<-round(lm_cv$results$Rsquared, digits=4)
```

#### 1.1.3 Test performance 

The next step is to check how good the model prediciton for the holdout are. Overall we see that for the lower prices houses the models seems to be working well. Howver, to predict the prices for more expensive houses the model tends to underestimate the price.

```{r linear model test, warning=FALSE, message=FALSE}
test_lm<-predict(lm, newdata = whole_data$holdout)

df_test<-cbind(df_test, test_lm)


ggplot(df_test, aes(x=price,y=test_lm))+
      geom_point(alpha=0.5)+xlab('actual')+ylab('predictions')+
      #+ylim(0,2000000)+xlim(0,2000000)
      geom_abline(slope=1, intercept=0)+
      ggtitle('Linear Regression - Test prediction')
```


#### 1.1.4 Residuals

Looking at the residuals supports our findings from the previous two plots. Thus this model would definetly need further improvement.

```{r linear model test residuals}
residualPlot(lm, ask=F)
```

#### 1.1.5 Result overview

Finally the results are calculated and saved in the result dataframe. The MAPE (our main perfomance measure in this case) lies at 0.19.

```{r linear model results}
result[1,3]<-round(rmse(real=whole_data$holdout$price, predicted = test_lm), digits=0)
result[1,4]<-round(mae(real=whole_data$holdout$price, predicted = test_lm), digits=0)
result[1,5]<-round(mape(real=whole_data$holdout$price, predicted = test_lm), digits=4)
datatable(result)
```


### 1.2 Linear regression model with regularization (Lasso)
The next model in line is a linear regression model with regularization. In this case we use a lasso regression which is a shrinkage and variable selection method for linear regressions. The objective of this method is to obtain a subset of features that minimizes the prediction error for the target variable.

#### 1.2.1 Trained linear model with regularization
The first step is to find lambda, which is a numeric valuedefining the amount of shrinkage. The best value for lambda is the one that minimzes the cv-preciction error rate. To do so cv.glmnet is used from the glmnet library.

```{r glmnet cv, warning=FALSE, message=FALSE}
glmnet_cv<-cv.glmnet(x = data.matrix(whole_data$train[, !'price']),
                     nfolds = 5,
                     y = whole_data$train[['price']],
                     alpha=1,
                     family = 'gaussian',
                     standardize = T)
plot.cv.glmnet(glmnet_cv)

glmnet_cv$lambda.min
```

In order to compute now the lasso regression lambda.min is used as shrinkage parameter. We obtain a R2 score of 0.817 which is slightly better than the one for the simple linear regression.

```{r glmnet , warning=FALSE, message=FALSE}
glmnet_0<-glmnet(x = data.matrix(whole_data$train[, !'price']), 
                 y = whole_data$train[['price']],
                 family = 'gaussian',
                 alpha=1, lambda = glmnet_cv$lambda.min)

glmnet_0
glmnet_0$beta
result[2,2]<-round(glmnet_0$dev.ratio, digits=4)
```

#### 1.2.2 Cross validation

There is no cross validation performed here since the minimum lambda was already obtain by using cross-validation.

#### 1.2.3 Test performance 

The next step is to make predictions for our holdout data set. Here we see a similar prediction behavior to the former model. For lower priced houses the model predict much better than for more expensive ones (+2 mio).

```{r glmnet test, warning=FALSE, message=FALSE}
test_glmnet<-predict(glmnet_0, newx = as.matrix(whole_data$holdout[, !'price']))


df_test<-cbind(df_test, test_glmnet=test_glmnet[,1])



ggplot(df_test, aes(x=price,y=test_glmnet))+
      geom_point(alpha=0.5)+xlab('actual')+ylab('predictions')+
      #+ylim(0,2000000)+xlim(0,2000000)
      geom_abline(slope=1, intercept=0)+
      ggtitle('Lasso Regression - Test prediction')
```

#### 1.2.4 Residuals

Looking at the residuals supports our findings from the previous two plots eventhough it seems like the residuals are overall closer to zero than in the linear model. Nevertheless this model would definetly need further improvement.
To plot the residuals the plotmo library was used. 

```{r glmnet test residuals}
plotres(glmnet_0, which=3)
```

#### 1.2.5 Result overview

Finally the results are calculated and saved in the result dataframe. The MAPE (our main perfomance measure in this case) lies around 0.19 as well - slightly below (=better) the result of the simple linear regression.

```{r glmnet results}
result[2,3]<-round(rmse(real=whole_data$holdout$price, predicted = test_glmnet), digits=0)
result[2,4]<-round(mae(real=whole_data$holdout$price, predicted = test_glmnet), digits=0)
result[2,5]<-round(mape(real=whole_data$holdout$price, predicted = test_glmnet), digits=4)
datatable(result)
```


### 1.3 Random forest model
The next model in line is random forest. In order to work with random forest, the ranger library is used. Random forest operates by constructing many decision trees and then taking the mean prediciton of all individual trees.


#### 1.3.1 Trained random forest model
The trained random forest model constructed 500 trees and used 105 independent variables. The R2 score lies at 0.854 which is higher than the two previous models.

```{r random forest, warning=FALSE, message=FALSE}
rf<-ranger(formula, whole_data$train)

summary(rf)

result[3,2]<-round(rf$r.squared, digits=4)
```

#### 1.3.2 Cross validation

Due to time constraints no cross validation is applied for this model at this point.

```{r random forest CV, warning=FALSE, message=FALSE}
#set.seed(123) 
#rf_cv <- rgcv(whole_data$train[, !'price'], whole_data$train$price, cv.fold = 5)
#rf_cv1
```

#### 1.3.3 Test performance 

The next step is to check how good the model prediciton for the holdout are. Overall we see that the predicted values and actual values are closer together than for the previous two models. However, the model has still problems to predict expensive houses accurately.

```{r random forest test, warning=FALSE, message=FALSE}
test_rf<-predict(rf,whole_data$holdout)$predictions

df_test<-cbind(df_test, test_rf)


ggplot(df_test, aes(x=price,y=test_rf))+
      geom_point(alpha=0.5)+xlab('actual')+ylab('predictions')+
      #+ylim(0,2000000)+xlim(0,2000000)
      geom_abline(slope=1, intercept=0)+
      ggtitle('Random Forest - Test prediction')
```


#### 1.3.4 Residuals

Also by looking at the residuals we see again that the model performs better in the lower price region than in the high price region.

```{r random forest test residuals}
rf_residuals <- whole_data$train$price - rf$predictions

residual_plot<-as.data.frame(whole_data$train$price)
residual_plot<-cbind(residual_plot,rf_residuals)

ggplot(residual_plot, aes(x=whole_data$train$price, rf_residuals)) +
    geom_point(shape=1) +
    geom_smooth()+
    theme_minimal()+
    ylab('Residuals')+
    xlab('Fitted values')
```

#### 1.3.5 Result overview

Finally the results are calculated and saved in the result dataframe. The MAPE (our main perfomance measure in this case) lies at 0.14, which is significantly lower than the two linear models. Also by looking at the other three performance measures we see that the random forest model outperforms the other two models.

```{r random forest results}
result[3,3]<-round(rmse(real=whole_data$holdout$price, predicted = test_rf), digits=0)
result[3,4]<-round(mae(real=whole_data$holdout$price, predicted = test_rf), digits=0)
result[3,5]<-round(mape(real=whole_data$holdout$price, predicted = test_rf), digits=4)
datatable(result)
```


### 1.4 Boosting trees model
The last baseline model is a tree based xgboost model. 


#### 1.4.1 Trained random forest model
The model uses cross validation with 5 folds. Since R2 is not a default parameter for xgboost but only RMSE it was not possible to construct it from the model. I tried to access the training predictions of the model but I did not manage to find it. We can not use the model to predict on the train data and then use these predicitons because the model was trained on it. The R2 result would lie around 0.96 (I tried it). Thus for now the R2 will remain empty.

```{r xgboost, warning=FALSE, message=FALSE}
xgb_trcontrol = trainControl(
  method = "cv",
  number = 5,  
  allowParallel = TRUE,
  verboseIter = FALSE,
  returnData = FALSE
)

xgb<-xgboost(booster='gbtree',
               data=as.matrix(whole_data$train[, !'price', with=F]),
               label=whole_data$train$price,
               nrounds = 50,
               objective='reg:linear',
               trControl = xgb_trcontrol)
print(xgb)

#training_predictions = ???
#res <- caret::postResample(whole_data$train$price, training_predictions)
#rsq <- res[2]
#result[4,2]<-round(rsq, digits=4)
result[4,2]<-NA



```

#### 1.4.2 Cross validation

Cross validation is already included in the trained model.


#### 1.4.3 Test performance 

The next step is to check how good the model prediciton for the holdout are. It seems that the model slightly improved here but still the problem for expensive houses remains.

```{r xgboost test, warning=FALSE, message=FALSE}
test_xgb<-predict(xgb, newdata = as.matrix(whole_data$holdout[, !'price', with=F]), type='response')


df_test<-cbind(df_test, test_xgb)


ggplot(df_test, aes(x=price,y=test_xgb))+
      geom_point(alpha=0.5)+xlab('actual')+ylab('predictions')+
      #+ylim(0,2000000)+xlim(0,2000000)
      geom_abline(slope=1, intercept=0)+
      ggtitle('XGBoost - Test prediction')
```


#### 1.4.4 Residuals

Since I cannot access the training predicitons it is not possible to draw a residual plot for the training model.

#### 1.4.5 Result overview

Finally the results are calculated and saved in the result dataframe. The MAPE (our main perfomance measure in this case) lies at 0.13, which is even lower than the random forest model. Also for the two other parameter the results of the xgboost model are better.

```{r xgboost results}
result[4,3]<-round(rmse(real=whole_data$holdout$price, predicted = test_xgb), digits=0)
result[4,4]<-round(mae(real=whole_data$holdout$price, predicted = test_xgb), digits=0)
result[4,5]<-round(mape(real=whole_data$holdout$price, predicted = test_xgb), digits=4)
datatable(result)
```

### 1.5 Baseline conclusion

Comparing the four models we see that the best baseline is achieved by using an boosting tree model (xgboost). Thus for feature engineering all features will be evaluated by using the xgboost model. In a final step hyperparameter tuning will be performed for xgboost.

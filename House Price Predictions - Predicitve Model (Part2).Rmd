---
title: "House Price Predictions - Predicitive Model (Part2)"
author: "Christine Utendorf"
date: "23 May 2019"
output: html_document
---

## 0. Sources and data loading

#### Notebook loading
Besides several libraries used for data exploration and visualization, the library loding also includes MASS, caret and ranger for the machine learning models.

All self-programed functions such as transforming or splitting a dataset are saved within the R notbook called functions.

Furthermore, several metrics that are not default for machine learning R packages are created as functions within the R notebook metrics. 
```{r, warning=FALSE, message=FALSE}
source('notebooks/libraries.R')
source('notebooks/functions.R')
source('notebooks/metrics.R')
```

The work is divided into two R markdowns. The first one only focusses on data exploration and preparation and the second one on modeling. This is due to the fact that each markdown takes some while to be created and with this division the whole data exploration does not need to be computed again while modeling.

#### Data loading
From the data preparation and exploration part the prepared train and test data sets are loaded.

```{r}
train_data<-fread('data/data_train_dummy.csv', stringsAsFactors = F)
test_data<-fread('data/data_test_dummy.csv', stringsAsFactors = F)
```

#### Splitting dataset
Since our test set is blind and we want to compare models when tuning and selecting them, a list object is created that not only stores the blind test data set but also a train data set (80% of train data) and a holdout set (20% of train data) that was created from the overall provided train data. In order to evaluate and monitor the performance of the models also k-folds cross-validation is performed on the 80% train data set. Furthermore the predictions on the holdout data set are evaluated in order to see how well the model performs on data that is has nevern been trained on before. Crossvalidation results might already be enough in this case to evaluate but since we also want to track non-default measures such as MAPE it is easier to calculated them on a holdout prediction set.

Since we want to predict the price, which is our target variable, this is stored in a variable called formular that uses price as target and all other variables of the data set as features, which are all turned into numeric variables. 


We store the id and the price data for the holdout in a seperate data set in order to calculate later the metrics. Furthermore also the ids of the blind test set are stored in a seperate dataset to later on match the final predictions for submission.

Id is then excluded from all datasets since there were severe problems when trying to predict with the models and not using the id as an input.

Before starting to model an empty data frame is created to store the results.

```{r}
whole_data = f_split(train_data, test_data, testsize = 0.2, seed = 1)
whole_data<-lapply(whole_data, function(x){
  return(x[, which(sapply(x, is.integer)):=lapply(.SD, as.numeric), .SDcols=sapply(x,is.integer)])
})

#df_test<-whole_data$holdout[,c("id", "price")]
df_test<-whole_data$holdout[, .(id=1:.N, price)]
df_pred<-whole_data$test[,"id"]

whole_data$train$id = NULL
whole_data$test$id = NULL
whole_data$holdout$id = NULL

formula<-as.formula(price~.)

result<-data.table(method=c('lm','glmnet','tree','rf','xgb','xgb_reg'),R2cv=NA, RMSE=NA, MAE=NA,MAPE=NA)
```


## 1. Linear model
Since we are predicting a continous target, it makes sense to start with a linear regression. Eventhough we have many features now due to dummy encoding, linear regression gives a good starting point for the modeling. Thus we will use it as a baseline and try to improve our results from there.

The first step is to use a simple linear model. It results in an adjusted R2 of 0.81 with many significant variables. However, several features show up as NAs. An explaination for this can be that those variables are highly correlated to other input variables and are right away discarded in the model.

```{r, warning=FALSE, message=FALSE}
lm <- lm(formula = formula, 
                 data=whole_data$train)
summary(lm)
```

In order to further evaluate the model a k-fold crossvaldiation is performed with 5 folds. The R2 for the cross valdiation is 0.807 which is quiet close to our actual R2 from the trained model and thus the model is not tending to overfit.

```{r, warning=FALSE, message=FALSE}
set.seed(123) 
train.control <- trainControl(method = "cv", number = 5)


lm_cv <- train(formula, data = whole_data$train, method = "lm",
               trControl = train.control)

print(lm_cv)

R2cv_lm<-lm_cv$results$Rsquared
```

The next step is to check how good the model prediciton for the holdout are. In this case we can also use the functions created to evaluate metrics such as MAE, MAPE and RMSE.

```{r, warning=FALSE, message=FALSE}
test_lm<-predict(lm, newdata = whole_data$holdout)

df_test<-cbind(df_test, test_lm)

str(df_test)

ggplot(df_test, aes(x=price,y=test_lm))+
      geom_point(alpha=0.5)+xlab('actual')+ylab('predictions')+
      #+ylim(0,2000000)+xlim(0,2000000)
      geom_abline(slope=1, intercept=0)+
      ggtitle('Linear Regression - Test prediction')

ggplot(melt(df_test, id.vars = 'id'), aes(x=id,y=value, colour=variable))+
  geom_point(alpha=0.65)+geom_line(alpha=0.65)+
  xlab('')+ylab('$')+
  ggtitle('lm - Test Prediction on House Price')+
  scale_colour_manual(values = c('black','red'))

residualPlot(lm, ask=F)


rmse_lm<-rmse(real=whole_data$holdout$price, predicted = test_lm)
mae_lm<-mae(real=whole_data$holdout$price, predicted = test_lm)
mape_lm<-mape(real=whole_data$holdout$price, predicted = test_lm)
mape_lm
```
